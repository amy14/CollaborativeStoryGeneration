# CollaborativeStoryGeneration
Story Collaborative Generation - Language Models for Story Generation using Entailment. Working under Noah Smith, Yangfeng Ji, Elizabeth Clark, and Maarten Sap in the University of Washington - Natural Language Processing Lab.


Improving upon the task of Collaborative Story Generation, the basic encoder decoder model with attention performs well to generate large amounts of text, however when we consider elements of writing such as coherence the model is lacking since it produces sentences that are too generic, repetitive or self-contradictory. Acting on the feedback received from human evaluation of the model, entailment seems a promising direction to encode the relationship between pairs of sentences in the story, while balancing the element of surprise and coherence of the next generated sentence. For a hypothesis and premise, the entailment categories include: entailment, neutral, contradiction, and reverse-entailment. Evaluating the entailment model on the SNLI corpus we found the model picked up on some annotation artifacts. In order to evaluate the model without the presence of these annotation artifacts we used the SNLI Hard and MultiNLI fiction genre data sets where the annotation artifacts are less discriminatory.
